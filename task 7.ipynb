{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10506362,"sourceType":"datasetVersion","datasetId":6504217}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import InformerModel\nfrom sklearn.metrics import classification_report, balanced_accuracy_score\nimport numpy as np\nimport os\nfrom sklearn.decomposition import PCA\nfrom scipy.signal import welch, coherence\nfrom scipy.stats import skew, kurtosis\nimport pywt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import InformerConfig, InformerForPrediction\n\nclass InformerClassifier(nn.Module):\n    def __init__(self, input_size=19, prediction_length=1, context_length=1, d_model=64, num_classes=4):\n        super(InformerClassifier, self).__init__()\n        \n        # Create Informer configuration\n        config = InformerConfig(\n            input_size=input_size,\n            prediction_length=prediction_length,\n            context_length=context_length,\n            d_model=d_model\n        )\n        \n        # Initialize Informer model with the configuration\n        self.informer = InformerForPrediction(config)\n        \n        # Classification head\n        self.fc = nn.Linear(d_model, num_classes)\n    \n    def forward(self, x):\n        # Assuming `x` contains the required tensors for Informer\n        outputs = self.informer(\n            past_values=x[\"past_values\"],\n            past_time_features=x[\"past_time_features\"],\n            past_observed_mask=x[\"past_observed_mask\"]\n        )\n        # Use the hidden states for classification\n        hidden_state = outputs.last_hidden_state\n        logits = self.fc(hidden_state)\n        return logits\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom scipy.signal import welch, coherence\nfrom scipy.stats import skew, kurtosis\nimport pywt\nfrom sklearn.decomposition import PCA\n\ndef extracted_features(data, sampling_rate=256, window_size=50):\n    num_channels, timesteps = data.shape\n    num_windows = timesteps // window_size  # Number of windows per signal\n    extracted_features = []\n\n    for w in range(num_windows):\n        window_features = []  # Store features for all channels in this window\n        \n        for channel in range(num_channels):\n            window = data[channel, w * window_size:(w + 1) * window_size]\n\n            # **Statistical Features**\n            mean = np.mean(window)\n            variance = np.var(window)\n            rms = np.sqrt(np.mean(window**2))\n            skewness = skew(window)\n            kurt = kurtosis(window)\n            zero_crossings = np.sum(np.diff(np.sign(window)) != 0)\n\n            # **Power Spectral Density (Frequency Features)**\n            freqs, psd = welch(window, fs=sampling_rate, nperseg=window_size)\n            delta_power = np.sum(psd[(freqs >= 0.5) & (freqs < 4)])\n            theta_power = np.sum(psd[(freqs >= 4) & (freqs < 8)])\n            alpha_power = np.sum(psd[(freqs >= 8) & (freqs < 12)])\n            beta_power = np.sum(psd[(freqs >= 13) & (freqs < 30)])\n            gamma_power = np.sum(psd[(freqs >= 30) & (freqs < 100)])\n\n            # **Wavelet Transform Features**\n            coeffs = pywt.wavedec(window, wavelet='db4', level=2)\n            wavelet_features = [np.mean(c) for c in coeffs] + [np.std(c) for c in coeffs]\n\n            # Collect all features\n            channel_features = [mean, variance, rms, skewness, kurt, zero_crossings,\n                                delta_power, theta_power, alpha_power, beta_power, gamma_power] + wavelet_features\n            \n            window_features.append(channel_features)  # Append channel features\n\n        extracted_features.append(window_features)  # Append window features\n\n    extracted_features = np.array(extracted_features)  # Shape: (num_windows, num_channels, num_features)\n    extracted_features = extracted_features.reshape(num_windows, -1)  # Flatten channels into feature vector\n    print(f\"Extracted features shape: {extracted_features.shape}\")\n    \n    return extracted_features  # Shape: (num_windows, total_features_per_window)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_split_data(split_name, base_data_path, folders, class_mapping):\n    split_path = os.path.join(base_data_path, split_name)\n    X, y = [], []\n    for folder in folders:\n        folder_path = os.path.join(split_path, folder)\n        if not os.path.exists(folder_path):\n            print(f\"Folder {folder_path} does not exist.\")\n            continue\n        \n        files = [f for f in os.listdir(folder_path) if f.endswith('.npy')]\n        if not files:\n            print(f\"Folder {folder_path} is empty.\")\n            continue\n        \n        for file in files:\n            file_path = os.path.join(folder_path, file)\n            try:\n                data = np.load(file_path).astype(np.float32)\n                if data.shape == (19, 500):  # Ensure consistent shape\n                    features = extracted_features(data)\n                    X.append(features)\n                    y.append(class_mapping[folder])\n            except Exception as e:\n                print(f\"Error loading {file_path}: {e}\")\n    \n    X, y = np.array(X), np.array(y)\n    print(f\"Loaded {split_name} data shape: {X.shape}, Labels shape: {y.shape}\")\n    return X, y","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load datasets\nbase_data_path = '/kaggle/input/impppp/Impulse/EEG_Data'\ndata_splits = ['train_data', 'validation_data']\nfolders = ['Normal', 'Complex_Partial_Seizures', 'Electrographic_Seizures', 'Video_detected_Seizures_with_no_visual_change_over_EEG']\nclass_mapping = {folder: idx for idx, folder in enumerate(folders)}\n\nX_train, y_train = load_split_data('train_data', base_data_path, folders, class_mapping)\nX_val, y_val = load_split_data('validation_data', base_data_path, folders, class_mapping)\n\nsampled_indices = np.random.choice(len(X_train), size=int(0.1 * len(X_train)), replace=False)\nX_train_sampled, y_train_sampled = X_train[sampled_indices], y_train[sampled_indices]\nprint(f\"Shape of X_train_sampled: {X_train_sampled.shape}\")\n\n\n# Create Dataloaders\ntrain_dataset = EEGDataset(X_train_sampled, y_train_sampled)\nval_dataset = EEGDataset(X_val, y_val)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\nprint(f\"Data prepared: Train size {len(train_dataset)}, Validation size {len(val_dataset)}\")\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EEGDataset(Dataset):\n    def __init__(self, X, y=None, sampling_rate=256):\n        \"\"\"\n        X: (samples, timesteps, features)\n        y: Labels\n        \"\"\"\n        self.past_values = torch.tensor(X, dtype=torch.float32)  # Shape: (samples, timesteps, features)\n        self.past_observed_mask = torch.ones_like(self.past_values, dtype=torch.float32)\n        self.labels = torch.tensor(y, dtype=torch.long) if y is not None else None\n\n        # Generate past_time_features\n        num_timesteps = X.shape[1]  # Assuming X is (samples, timesteps, features)\n        \n        # Example: Normalized time index\n        time_index = np.linspace(0, 1, num_timesteps)\n        self.past_time_features = torch.tensor(time_index, dtype=torch.float32).unsqueeze(0).repeat(len(X), 1).unsqueeze(-1)\n        \n        # You can add more features to this if needed.\n    \n    def __len__(self):\n        return len(self.past_values)\n\n    def __getitem__(self, idx):\n        data = {\n            \"past_values\": self.past_values[idx],  # (timesteps, features)\n            \"past_observed_mask\": self.past_observed_mask[idx],  # (timesteps, features)\n            \"past_time_features\": self.past_time_features[idx]  # (timesteps, num_features)\n        }\n        if self.labels is not None:\n            data[\"labels\"] = self.labels[idx]\n        return data\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=1e-3, device='cuda' if torch.cuda.is_available() else 'cpu'):\n    \"\"\"\n    Trains the InformerClassifier model.\n\n    Args:\n        model: The model to be trained.\n        train_loader: DataLoader for training data.\n        val_loader: DataLoader for validation data.\n        num_epochs: Number of epochs to train.\n        learning_rate: Learning rate for optimizer.\n        device: Device to train on ('cuda' or 'cpu').\n\n    Returns:\n        Trained model.\n    \"\"\"\n    # Move model to device\n    model = model.to(device)\n    \n    # Define optimizer and loss function\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    criterion = nn.CrossEntropyLoss()\n    \n    # Training loop\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        for batch in train_loader:\n            # Move data to device\n            past_values = batch['past_values'].to(device)\n            past_time_features = batch['past_time_features'].to(device)\n            past_observed_mask = batch['past_observed_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            # Zero the gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            logits = model({\n                \"past_values\": past_values,\n                \"past_time_features\": past_time_features,\n                \"past_observed_mask\": past_observed_mask\n            })\n            \n            # Compute loss\n            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n            \n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n        \n        # Validation loop\n        model.eval()\n        val_loss = 0.0\n        all_preds = []\n        all_labels = []\n        with torch.no_grad():\n            for batch in val_loader:\n                past_values = batch['past_values'].to(device)\n                past_time_features = batch['past_time_features'].to(device)\n                past_observed_mask = batch['past_observed_mask'].to(device)\n                labels = batch['labels'].to(device)\n                \n                logits = model({\n                    \"past_values\": past_values,\n                    \"past_time_features\": past_time_features,\n                    \"past_observed_mask\": past_observed_mask\n                })\n                \n                loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n                val_loss += loss.item()\n                \n                preds = torch.argmax(logits, dim=-1).cpu().numpy()\n                all_preds.extend(preds)\n                all_labels.extend(labels.cpu().numpy())\n        \n        # Compute metrics\n        train_loss /= len(train_loader)\n        val_loss /= len(val_loader)\n        balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n        \n        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n        print(f\"Train Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f} | Balanced Accuracy: {balanced_acc:.4f}\")\n    \n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = InformerClassifier(input_size=X_train_sampled.shape[1])\nprint(f\"past_values shape: {past_values.shape}\")\nprint(f\"past_time_features shape: {past_time_features.shape}\")\ntrained_model = train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=1e-3)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}